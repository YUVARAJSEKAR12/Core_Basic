{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPILoX0kdiLmR6JfmXhMwpo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N44dWb5_iuLS","executionInfo":{"status":"ok","timestamp":1734172139906,"user_tz":-330,"elapsed":3282,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"1005d9b4-6527-4554-b725-c7e6909069e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fuzzy in /usr/local/lib/python3.10/dist-packages (1.2.2)\n"]}],"source":["pip install fuzzy"]},{"cell_type":"code","source":[],"metadata":{"id":"Pnb4DU2Kqyov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTIo5xlFjFny","executionInfo":{"status":"ok","timestamp":1734174214995,"user_tz":-330,"elapsed":416,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"ac53d502-85aa-47e0-ba9f-8e25a17ee492"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from fuzzywuzzy import fuzz\n","from fuzzywuzzy import process\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","def electro():\n","  return \"\"\"Electromagnetism is a fundamental physical phenomenon that describes\n","          the interaction between electrically charged particles and the\n","          electromagnetic force, one of the four fundamental forces of nature.\"\"\"\n","  languagespecified=input(\"Enter a string = \")\n","  llanguagespecified=languagespecified.lower()\n","  print(\"Language specified is: \",llanguagespecified)\n","  if llanguagespecified == \"hindi\":\n","    t = Translator()\n","    translated_text = t.translate(\"\", src='en', dest='hi')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"spanish\":\n","    t = Translator()\n","    translated_text = t.translate(\"\", src='en', dest='es')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"french\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='fr')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"german\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='de')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"italian\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='it')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"portuguese\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='pt')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"japanese\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='ja')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"korean\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='ko')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"chinese\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='zh-cn')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"russian\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='ru')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"arabic\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='ar')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"tamil\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='ta')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"telugu\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='te')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"bengali\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='bn')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"urdu\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='ur')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"gujarati\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='gu')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"marathi\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='mr')\n","    b=translated_text.text\n","    print(b)\n","  elif llanguagespecified == \"kannada\":\n","    t = Translator()\n","    translated_text = t.translate(\"Hello, how are you?\", src='en', dest='kn')\n","    b=translated_text.text\n","    print(b)\n","  else:\n","    print(\"Language not supported\")\n","\n","d=electro()\n","\n","ga=[\"could\",\"you\",\"provide\",\"a\",\"simple\",\"explanation\",\"of\",\"can\",\"break\",\"down\",\n","   \"the\",\"concept\",\"for\",\"me\",\"what\",\"exactly\",\"is\",\"explain\",\"how\",\"does\",\n","   \"work\",\"why\",\"we\",\"need\",\"definition\",\"tell\",\"me\",\"about\",\"describe\",\n","   \"called\",\"state\",\"define\",\"electromagnetic\",\"hi\",\"?\"]\n","\n","ip=input(\"Ente the question?\")\n","# Tokenize the input sentence\n","tokens = word_tokenize(ip)\n","\n","# Get English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Add custom stopwords\n","additional_stopwords = [\"leading\", \"platform\", \"building\",\"hello\"]\n","stop_words.update(additional_stopwords)\n","\n","# Remove stopwords\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","result=\"\".join(filtered_tokens)\n","print(result)\n","e1=\"electro\"\n","\n","z=result\n","\n","z1=\"electromagnetism\"\n","\n","r1=fuzz.ratio(z,z1)\n","print(\"Ratio : \",r1)\n","\n","print(\"Original Sentence:\")\n","print(ip)\n","print(\"\\nFiltered Sentence:\")\n","print(result)\n","\n","if r1 >80:\n","   print(translate())\n","if(result==e1):\n","   print(translate())\n",""],"metadata":{"id":"vtGkNsEsygFn","colab":{"base_uri":"https://localhost:8080/","height":766},"executionInfo":{"status":"error","timestamp":1734176194899,"user_tz":-330,"elapsed":15716,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"7ffae15d-cd2f-42b3-f637-e20c5256caa6"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Enter the language ta\n","Ente the question?hello electroVmagnetism\n","electroVmagnetism\n","Ratio :  97\n","Original Sentence:\n","hello electroVmagnetism\n","\n","Filtered Sentence:\n","electroVmagnetism\n","Set the destination language\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'Translator' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-017d5f4e923b>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-017d5f4e923b>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the language \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Set the destination language\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32mreturn\u001b[0m  \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Translator' is not defined"]}]},{"cell_type":"code","source":["pip install rapidfuzz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVcGkcj4vOrI","executionInfo":{"status":"ok","timestamp":1734172572833,"user_tz":-330,"elapsed":6698,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"1c02c920-b341-424a-de06-fc76c1ff4cf3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rapidfuzz\n","  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/3.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz\n","Successfully installed rapidfuzz-3.10.1\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Ensure NLTK's tokenizer is downloaded\n","nltk.download('punkt')\n","\n","a = \"Hi how are you\"\n","tokens = word_tokenize(a)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hsvis1DXjP2w","executionInfo":{"status":"ok","timestamp":1733716416398,"user_tz":-330,"elapsed":377,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"4a34487e-4ba6-48bc-e351-73118332d5b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hi', 'how', 'are', 'you']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","text = \"hi I am balamurugan\"\n","filter = (\"hi\", \"i\", \"am\")\n","\n","tokens = word_tokenize(text)\n","filtered_tokens = [token for token in tokens if token not in filter]\n","\n","filtered_text = ' '.join(filtered_tokens)\n","print(filtered_text)"],"metadata":{"id":"9Bdc_8Rgj76k","executionInfo":{"status":"ok","timestamp":1733716594819,"user_tz":-330,"elapsed":387,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"776d8da7-6911-43d0-ec80-cbea118d6ec5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I balamurugan\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","text = \"hi I am balamurugan\"\n","filter = (\"hi\", \"i\", \"am\")\n","\n","tokens = word_tokenize(text)\n","filtered_tokens = [token for token in tokens if token not in filter]\n","\n","filtered_text = ' '.join(filtered_tokens)\n","print(filtered_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyGu8rBkx4sd","executionInfo":{"status":"ok","timestamp":1733888121265,"user_tz":-330,"elapsed":367,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"454373f3-ee98-4230-c06d-235734af90a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I balamurugan\n"]}]},{"cell_type":"code","source":["#filter\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Download NLTK stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Define the input sentence\n","input_sentence = input(\"Enter =\")\n","\n","# Tokenize the input sentence\n","tokens = word_tokenize(input_sentence)\n","\n","# Get English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Remove stopwords\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","# Join the filtered tokens back into a sentence\n","filtered_sentence = \" \".join(filtered_tokens)\n","\n","print(\"Original Sentence:\")\n","print(input_sentence)\n","print(\"\\nFiltered Sentence:\")\n","print(filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4sznlvf4yhIb","executionInfo":{"status":"ok","timestamp":1733888220553,"user_tz":-330,"elapsed":13875,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"aaba12ad-65b7-451d-9a97-0b46a8c9a5f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Enter =Hi This is Yuvaraj\n","Original Sentence:\n","Hi This is Yuvaraj\n","\n","Filtered Sentence:\n","Hi Yuvaraj\n"]}]},{"cell_type":"code","source":["#add words\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Download NLTK stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Define the input sentence\n","input_sentence = input(\"Enter =\")\n","\n","# Tokenize the input sentence\n","tokens = word_tokenize(input_sentence)\n","\n","# Get English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Add custom stopwords\n","additional_stopwords = [\"leading\", \"platform\", \"building\",\"hi\"]\n","stop_words.update(additional_stopwords)\n","\n","# Remove stopwords\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","# Join the filtered tokens back into a sentence\n","filtered_sentence = \" \".join(filtered_tokens)\n","\n","print(\"Original Sentence:\")\n","print(input_sentence)\n","print(\"\\nFiltered Sentence:\")\n","print(filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1YotPre1k8I","executionInfo":{"status":"ok","timestamp":1733889004523,"user_tz":-330,"elapsed":11108,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"38286e24-5341-4021-f8de-34ab50398b24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Enter =enter the data\n","Original Sentence:\n","enter the data\n","\n","Filtered Sentence:\n","enter data\n"]}]},{"cell_type":"code","source":["#remove words\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Download NLTK stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Define the input sentence\n","input_sentence = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n","\n","# Tokenize the input sentence\n","tokens = word_tokenize(input_sentence)\n","\n","# Get English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Remove specific words from the stopwords list\n","words_to_remove = [\"is\", \"a\", \"for\", \"to\", \"with\"]\n","for word in words_to_remove:\n","    stop_words.remove(word)\n","\n","# Remove stopwords\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","# Join the filtered tokens back into a sentence\n","filtered_sentence = \" \".join(filtered_tokens)\n","\n","print(\"Original Sentence:\")\n","print(input_sentence)\n","print(\"\\nFiltered Sentence:\")\n","print(filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPGnPPBI1wV5","executionInfo":{"status":"ok","timestamp":1733889038243,"user_tz":-330,"elapsed":378,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"3b64b483-abc1-4f26-cfb1-447bae283d2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence:\n","NLTK is a leading platform for building Python programs to work with human language data.\n","\n","Filtered Sentence:\n","NLTK is a leading platform for building Python programs to work with human language data .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["pip install googletrans==4.0.0-rc1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6x76NcF4WGk","executionInfo":{"status":"ok","timestamp":1733891394570,"user_tz":-330,"elapsed":6847,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"1cc92d12-7e53-48e1-fa62-e4e20b4f786a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans==4.0.0-rc1\n","  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n","Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=908ca3e64fc13a3604c5148149cce414690b185c0bcb23f84cdf6f4d5c537978\n","  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: h11\n","    Found existing installation: h11 0.14.0\n","    Uninstalling h11-0.14.0:\n","      Successfully uninstalled h11-0.14.0\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.7\n","    Uninstalling httpcore-1.0.7:\n","      Successfully uninstalled httpcore-1.0.7\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.28.0\n","    Uninstalling httpx-0.28.0:\n","      Successfully uninstalled httpx-0.28.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langsmith 0.1.147 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","openai 1.54.5 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]}]},{"cell_type":"code","source":["#en english es spanish\n","from googletrans import Translator\n","\n","t = Translator()\n","a = \"Hello, how are you?\"\n","translated_text = t.translate(a, src='en', dest='es')\n","b=translated_text.text\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0E-ofPE-yY4","executionInfo":{"status":"ok","timestamp":1733891429292,"user_tz":-330,"elapsed":723,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"3d167aae-1086-49c8-e2f2-02628f94af38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["¿Hola, cómo estás?\n"]}]},{"cell_type":"code","source":["#en english es spanish\n","from googletrans import Translator\n","\n","t = Translator()\n","a = \"Hello, how are you?\"\n","translated_text = t.translate(a, src='en', dest='ta')\n","b=translated_text.text\n","print(b)"],"metadata":{"id":"aRq9g5BP_Gok","executionInfo":{"status":"ok","timestamp":1733891488255,"user_tz":-330,"elapsed":432,"user":{"displayName":"Yuvaraj S","userId":"11707681052304735113"}},"outputId":"69177461-ed2b-4e2a-bec7-67b83b652cbf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்?\n"]}]}]}